{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SaintPlus.dataloader import Train_Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file) :\n",
    "    import yaml\n",
    "    \n",
    "    with open(config_file) as file :\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cfg = load_config('./config.yaml')\n",
    "    \n",
    "total_data = cfg['data_dir'] + cfg['total_data_name']\n",
    "total_df = pd.read_csv(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAdam'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madamW\u001b[39m\u001b[38;5;124m'\u001b[39m :\n\u001b[1;32m      9\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m NoamOpt(cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m, cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_step\u001b[39m\u001b[38;5;124m'\u001b[39m], optim\u001b[38;5;241m.\u001b[39mAdamW(lr \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moptimizer\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "from model import NoamOpt\n",
    "import torch.optim as optim\n",
    "\n",
    "if cfg['optimizer'] == 'adam' :\n",
    "    optimizer = NoamOpt(cfg['d_model'], 1, cfg['warmup_step'], optim.Adam(lr = cfg['lr']))\n",
    "elif cfg['optimizer'] == 'Nadam' :\n",
    "    optimizer = NoamOpt(cfg['d_model'], 1, cfg['warmup_step'], optim.NAdam(lr = cfg['lr']))\n",
    "elif cfg['optimizer'] == 'adamW' :\n",
    "    optimizer = NoamOpt(cfg['d_model'], 1, cfg['warmup_step'], optim.AdamW(lr = cfg['lr']))\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<model.NoamOpt object at 0x7efd86bc0190>\n"
     ]
    }
   ],
   "source": [
    "from model import NoamOpt\n",
    "import torch.optim as optim\n",
    "\n",
    "# 기본 옵티마이저를 먼저 정의합니다.\n",
    "base_optimizer = None\n",
    "\n",
    "if cfg['optimizer'] == 'adam':\n",
    "    base_optimizer = optim.Adam(lr=cfg['lr'])\n",
    "elif cfg['optimizer'] == 'Nadam':\n",
    "    base_optimizer = optim.NAdam(lr=cfg['lr'])\n",
    "elif cfg['optimizer'] == 'adamW':\n",
    "    base_optimizer = optim.AdamW(lr=cfg['lr'])\n",
    "\n",
    "# NoamOpt 초기화\n",
    "optimizer = NoamOpt(cfg['d_model'], 1, cfg['warmup_step'], base_optimizer)\n",
    "\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개인별 문제 푸는데 걸린 시간\n",
    "def get_time_lag(df) :\n",
    "    time_dict = {}\n",
    "    time_lag = np.zeros(len(df), dtype = np.float32)\n",
    "    for idx, col in enumerate(df[['userID', 'Timestamp', 'testId']].values) :\n",
    "        col[1] = time.mktime(datetime.strptime(col[1],'%Y-%m-%d %H:%M:%S').timetuple())\n",
    "        # 처음 문제 푸는 유저\n",
    "        if col[0] not in time_dict :\n",
    "            time_lag[idx] = 0\n",
    "            time_dict[col[0]] = [col[1], col[2], 0] # last_timestamp, last_task_container_id, last_lagtime\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            # 이 시험지를 풀어봤다면\n",
    "            if col[2] == time_dict[col[0]][1] :\n",
    "                time_lag[idx] = time_dict[col[0]][2]\n",
    "            \n",
    "            # 이 시험지를 푼 적 없다면\n",
    "            else :\n",
    "                time_lag[idx] = col[1] - time_dict[col[0]][0]\n",
    "                time_dict[col[0]][0] = col[1]\n",
    "                time_dict[col[0]][1] = col[2]\n",
    "                time_dict[col[0]][2] = time_lag[idx]\n",
    "\n",
    "    df['time_lag'] = time_lag/1000/60 # convert to miniute\n",
    "    # 문제푼지 하루가 지났다면 1440(60*24)로 만들어주고, 아니라면 그대로, 0보다 작다면 0으로 만들어줌\n",
    "    df['time_lag'] = df['time_lag'].clip(0, 1440) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 문제 푸는데 걸린 시간\n",
    "def duration(df):\n",
    "    df = df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df['months'] = df['Timestamp'].dt.month\n",
    "    df['days'] = df['Timestamp'].dt.day\n",
    "    df['ts'] = df['Timestamp'].map(pd.Timestamp.timestamp)\n",
    "    df['prev_ts'] = df.groupby(['userID', 'testId', 'months','days'])['ts'].shift(1)\n",
    "    df['prev_ts'] = df['prev_ts'].fillna(0)\n",
    "    df['elapsed'] = np.where(df['prev_ts'] == 0, 0, df['ts'] - df['prev_ts'])\n",
    "\n",
    "    indexes = df[df['elapsed'] > 1200].index\n",
    "    df.loc[indexes, 'elapsed'] = 1200\n",
    "    df = df.drop(['months','days','ts','prev_ts'],axis='columns')\n",
    "    return df\n",
    "\n",
    "# 문항별 평균\n",
    "def make_assess_ratio(df):\n",
    "    ratio_dict = defaultdict(float)\n",
    "    grouped_dict = dict(df.groupby('assessmentItemID')['answerCode'].value_counts())\n",
    "    assess_keys = list(set([x[0] for x in grouped_dict.keys()]))\n",
    "    for key in assess_keys:\n",
    "        if grouped_dict.get((key, 1)):\n",
    "            right = grouped_dict[(key, 1)]\n",
    "        else:\n",
    "            right=0\n",
    "        if grouped_dict.get((key, 0)):\n",
    "            wrong = grouped_dict[(key, 0)]\n",
    "        else:\n",
    "            wrong = 0\n",
    "        ratio = right / (right + wrong + 1e-10)\n",
    "        ratio_dict[key] = ratio\n",
    "\n",
    "    df['assessmentItemAverage'] = df['assessmentItemID'].map(ratio_dict)\n",
    "    return df\n",
    "\n",
    "# 유저별 평균\n",
    "def make_user_ratio(df):\n",
    "    ratio_dict = defaultdict(float)\n",
    "    grouped_dict = dict(df.groupby('userID')['answerCode'].value_counts())\n",
    "    user_keys = list(set([x[0] for x in grouped_dict.keys()]))\n",
    "    for key in user_keys:\n",
    "        if grouped_dict.get((key, 1)):\n",
    "            right = grouped_dict[(key, 1)]\n",
    "        else:\n",
    "            right = 0\n",
    "        if grouped_dict.get((key, 0)):\n",
    "            wrong = grouped_dict[(key, 0)]\n",
    "        else:\n",
    "            wrong = 0\n",
    "        ratio = right / (right + wrong + 1e-10)\n",
    "        ratio_dict[key] = ratio\n",
    "\n",
    "    df['UserAverage'] = df['userID'].map(ratio_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "def indexing(df, col) :\n",
    "    col2idx = {v : k for k, v in enumerate(df[col].unique())}\n",
    "    df[col] = df[col].map(col2idx)\n",
    "    return df\n",
    "\n",
    "def Feature_Engineering(train_df) :\n",
    "    print('Start Feature Engineering')\n",
    "    train_df.index = train_df.index.astype('uint32')\n",
    "    \n",
    "    # get time_lag feature\n",
    "    train_df = get_time_lag(train_df)\n",
    "    \n",
    "    # 문제 푼 시간\n",
    "    train_df = duration(train_df)\n",
    "    # 문제별 평균\n",
    "    train_df = make_assess_ratio(train_df)\n",
    "    # 유저별 평균\n",
    "    train_df = make_user_ratio(train_df)\n",
    "    # 문제번호 1부터 시작으로 바꾸기\n",
    "    train_df = indexing('assessmentItemID',train_df)\n",
    "    train_df['assessmentItemID'] += 1\n",
    "    # 시험지번호 1부터 시작으로 바꾸기\n",
    "    train_df = indexing('testId',train_df)\n",
    "    train_df['testId'] += 1\n",
    "    # 문제 맞추면 2, 틀리면 1로 바꾸기\n",
    "    train_df['answerCode'] += 1\n",
    "    print('Finish Feature Engineering')\n",
    "    return train_df\n",
    "\n",
    "def grouping(cfg, df, features, save_name) :\n",
    "    df_group = df[features].groupby('userID').apply(lambda df : (\n",
    "            df['assessmentItemID'].values,\n",
    "            df['testId'].values,\n",
    "            df['time_lag'].values,\n",
    "            df['elapsed'].values, \n",
    "            df['assessmentItemAverage'].values,\n",
    "            df['UserAverage'].values,\n",
    "            df['answerCode'].values\n",
    "        ))\n",
    "    with open(cfg['data_dir'] + f'{save_name}.pkl.zip', 'wb') as p :\n",
    "        pickle.dump(df_group, p)\n",
    "\n",
    "def Preprocess(cfg, df, is_train = True) :\n",
    "    print('Start Preprocess')\n",
    "    df = Feature_Engineering(df)\n",
    "    features = ['userID', 'assessmentItemID', 'testId', \n",
    "                'time_lag', 'elapsed',\n",
    "                'UserAverage', 'assessmentItemAverage',\n",
    "                'answerCode']\n",
    "    \n",
    "    # Train / Valid Data\n",
    "    if is_train :\n",
    "        valid_size = cfg['valid_size']\n",
    "        train_df = df[:int(df.shape[0] * valid_size)]\n",
    "        valid_df = df[int(df.shape[0] * valid_size):]\n",
    "        print(f'Train : {train_df.shape}, Valid : {valid_df.shape}')\n",
    "        print('=' * 50)\n",
    "\n",
    "        print('Start Train and Valid Data Grouping')\n",
    "        grouping(train_df, features, f'Train_Group_{valid_size * 100}')\n",
    "        grouping(valid_df, features, f'Valid_Group_{(1 - valid_size) * 100}')\n",
    "        print('Finish Preprocess')\n",
    "    \n",
    "    # Test Data\n",
    "    else :\n",
    "        print('Start Test Data Grouping')\n",
    "        grouping(df, features, 'Test_Group')\n",
    "        print('Finish Preprocess')\n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    cfg = load_config('./config.yaml')\n",
    "    \n",
    "    total_data = cfg['data_dir'] + cfg['total_data_name']\n",
    "    total_df = pd.read_csv(total_data)\n",
    "    Preprocess(cfg, total_df, True)\n",
    "    \n",
    "    test_data = cfg['data_dir'] + cfg['test_data_name']\n",
    "    test_df = pd.read_csv(test_data)\n",
    "    Preprocess(cfg, test_df, False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "level2-dkt-recsys-05-ywsvzukT-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
